{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ced0a4",
   "metadata": {},
   "source": [
    "# 02 â€” Preprocessing: BIO Alignment & Tokenization\n",
    "\n",
    "**Dataset:** `ai4privacy/open-pii-masking-500k-ai4privacy`  \n",
    "**Languages:** English (en), German (de), Italian (it), French (fr)  \n",
    "**Model/tokenizer:** `xlm-roberta-base`\n",
    "\n",
    "**What this notebook does**\n",
    "1) Load & filter dataset to en/de/it/fr  \n",
    "2) Detect span field, normalize to (start, end, label)  \n",
    "3) Build BIO tag set + mappings (`label2id`, `id2label`)  \n",
    "4) Align char-level spans â†’ token-level BIO using offsets  \n",
    "5) Save tokenized dataset to `data/hf_tokenized` + `data/labels.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462304ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'MODEL': 'xlm-roberta-base', 'LANGS': ['de', 'en', 'fr', 'it'], 'OUT': 'data'}\n"
     ]
    }
   ],
   "source": [
    "# Reproducible setup + paths\n",
    "import json, random  # JSON for saving/loading, random for reproducibility\n",
    "from pathlib import Path  # For file and directory paths\n",
    "from typing import List, Tuple  # For type hints\n",
    "import pandas as pd  # Data analysis and manipulation\n",
    "from datasets import load_dataset, DatasetDict  # Hugging Face datasets\n",
    "from transformers import AutoTokenizer  # Tokenizer for model\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Model and language settings\n",
    "MODEL = \"xlm-roberta-base\"\n",
    "LANGS = {\"en\", \"de\", \"it\", \"fr\"}\n",
    "OUT = Path(\"data\"); OUT.mkdir(parents=True, exist_ok=True)  # Output directory\n",
    "\n",
    "print(\"Settings:\", {\"MODEL\": MODEL, \"LANGS\": sorted(LANGS), \"OUT\": str(OUT)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbae0b2",
   "metadata": {},
   "source": [
    "**Reproducibility Note:**\n",
    "To ensure full reproducibility, we set the random seed for Python, NumPy, and PyTorch. This helps guarantee that results are consistent across runs, especially when using DataLoader shuffling or other stochastic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations\n",
    "import torch  # For deep learning\n",
    "np.random.seed(SEED)  # Set numpy seed\n",
    "torch.manual_seed(SEED)  # Set torch seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ded4f",
   "metadata": {},
   "source": [
    "### We use `xlm-roberta-base` because:\n",
    "\n",
    "Multilingual capability â€” Itâ€™s pretrained on 100+ languages, including English, German, Italian, and French, so it can represent all our target languages well.\n",
    "\n",
    "Tokenization robustness â€” Uses SentencePiece subword tokenization, which handles different scripts, diacritics, and word forms without needing separate vocabularies.\n",
    "\n",
    "Strong NER performance â€” XLM-R models are proven competitive in multilingual NER benchmarks (like XTREME and CoNLL), often outperforming older multilingual BERT.\n",
    "\n",
    "Good size/speed trade-off â€” base version (~270M parameters) balances accuracy and training speed, making it feasible to fine-tune on a single GPU.\n",
    "\n",
    "Hugging Face ecosystem support â€” Fully integrated with ðŸ¤— Transformers, which simplifies token classification training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6adc5",
   "metadata": {},
   "source": [
    "## 3. Load dataset\n",
    "\n",
    "We pull the dataset and keep only the target languages for **train/validation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9313b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012cd5f2686443499404f80f08a5a2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfacb2a3c8347ddab71a6e15da92e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1610c82113b84b419585215251517aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588e4fa2865848fba324a96f6f87cf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/464150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2523d74bac04cf89930aec863cbdd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/116077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
       "        num_rows: 464150\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
       "        num_rows: 116077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  # Import the function to load datasets\n",
    "ds_raw = load_dataset(\n",
    "    \"ai4privacy/open-pii-masking-500k-ai4privacy\",  # Dataset name on Hugging Face Hub\n",
    "    cache_dir=\"hf-cache\",                 # Use a local cache directory for downloads\n",
    "    download_mode=\"force_redownload\"      # Always redownload to avoid stale cache\n",
    ")\n",
    "ds_raw  # Show the loaded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b1233",
   "metadata": {},
   "source": [
    "### 4. Filtering by Language\n",
    "\n",
    "We filter the dataset to include only samples in our target languages: English (`en`), German (`de`), Italian (`it`), and French (`fr`). This ensures that all subsequent processing and modeling steps focus exclusively on these four languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb51137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a37832eb7c9439d89dd91c1140ab1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/116077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 331106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 82931\n",
      "    })\n",
      "})\n",
      "Columns: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes']\n",
      "Example: {'source_text': 'str', 'masked_text': 'str', 'privacy_mask': 'list', 'split': 'str', 'uid': 'int', 'language': 'str', 'region': 'str', 'script': 'str', 'mbert_tokens': 'list', 'mbert_token_classes': 'list'}\n"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": ds_raw[\"train\"].filter(lambda x: x[\"language\"] in LANGS),  # Keep only target languages in train\n",
    "    \"validation\": ds_raw[\"validation\"].filter(lambda x: x[\"language\"] in LANGS),  # ...and in validation\n",
    "})\n",
    "print(ds)  # Print dataset summary\n",
    "print(\"Columns:\", ds[\"train\"].column_names)  # Show available columns\n",
    "print(\"Example:\", {k: type(ds['train'][0][k]).__name__ for k in ds[\"train\"].column_names})  # Show types of fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd50d2",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "**Splits & Sizes**\n",
    "| Split       | # Samples |\n",
    "|-------------|-----------|\n",
    "| Train       | 331,106   |\n",
    "| Validation  | 82,931    |\n",
    "\n",
    "**Columns (Features)**\n",
    "| Column Name             | Type  | Description |\n",
    "|-------------------------|-------|-------------|\n",
    "| `source_text`           | str   | Original sentence/text |\n",
    "| `masked_text`           | str   | Text with PII replaced/masked |\n",
    "| `privacy_mask`          | list  | List of entity spans and labels |\n",
    "| `split`                 | str   | Dataset split indicator (train/validation) |\n",
    "| `uid`                   | int   | Unique identifier |\n",
    "| `language`              | str   | Language code (e.g., en, de, it, fr) |\n",
    "| `region`                | str   | Region code |\n",
    "| `script`                | str   | Writing system |\n",
    "| `mbert_tokens`          | list  | Tokens from mBERT tokenizer |\n",
    "| `mbert_token_classes`   | list  | Token-level labels for mBERT |\n",
    "\n",
    "**Purpose**\n",
    "- Multilingual dataset containing text with **personally identifiable information (PII)**.\n",
    "- Designed for **token classification** tasks such as NER (Named Entity Recognition) and PII masking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af778a1",
   "metadata": {},
   "source": [
    "## 5. Detect span field\n",
    "\n",
    "The dataset stores spans in a field like `privacy_mask`.  \n",
    "We autoâ€‘detect and normalize to a list of **(start, end, label)** triples using the `privacy_mask` field (or similar, e.g., `spans`, `entities`). This ensures compatibility across possible dataset schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aabfc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using span field: privacy_mask\n"
     ]
    }
   ],
   "source": [
    "CANDIDATE_SPAN_FIELDS = [\"privacy_mask\", \"spans\", \"entities\", \"span_labels\"]  # Possible span field names\n",
    "SPAN_FIELD = next((c for c in CANDIDATE_SPAN_FIELDS if c in ds[\"train\"].column_names), None)  # Find which exists\n",
    "assert SPAN_FIELD, f\"No span field found. Columns: {ds['train'].column_names}\"\n",
    "print(\"Using span field:\", SPAN_FIELD)\n",
    "\n",
    "def iter_spans(example) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"Return [(start, end, label), ...] regardless of the original schema.\"\"\"\n",
    "    items = example.get(SPAN_FIELD) or []  # Get the list of spans\n",
    "    triples = []\n",
    "    for it in items:\n",
    "        if isinstance(it, dict):  # If span is a dict, extract fields by key\n",
    "            triples.append((it[\"start\"], it[\"end\"], it[\"label\"]))\n",
    "        elif isinstance(it, (list, tuple)) and len(it) >= 3:  # If span is a tuple/list\n",
    "            triples.append((it[0], it[1], it[2]))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af875508",
   "metadata": {},
   "source": [
    "## 6. Build label set\n",
    "\n",
    "Collect all distinct entity labels across splits (en/de/it/fr only), then build BIO tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23fe7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found labels: ['AGE', 'BUILDINGNUM', 'CITY', 'CREDITCARDNUMBER', 'DATE', 'DRIVERLICENSENUM', 'EMAIL', 'GENDER', 'GIVENNAME', 'IDCARDNUM', 'PASSPORTNUM', 'SEX', 'SOCIALNUM', 'STREET', 'SURNAME', 'TAXNUM', 'TELEPHONENUM', 'TIME', 'TITLE', 'ZIPCODE']\n",
      "Num BIO classes (incl O): 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B-AGE                 0\n",
       "B-BUILDINGNUM         1\n",
       "B-CITY                2\n",
       "B-CREDITCARDNUMBER    3\n",
       "B-DATE                4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set()  # Collect unique entity labels\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    for ex in ds[split]:\n",
    "        for s, e, lab in iter_spans(ex):\n",
    "            labels.add(lab)  # Add each label found\n",
    "\n",
    "labels = sorted(labels)  # Sort for consistency\n",
    "print(\"Found labels:\", labels)\n",
    "\n",
    "# BIO mapping: B-*, I-* plus O\n",
    "label2id = {f\"B-{l}\": i for i, l in enumerate(labels)}  # Beginning of entity\n",
    "label2id.update({f\"I-{l}\": i + len(labels) for i, l in enumerate(labels)})  # Inside entity\n",
    "O_ID = len(label2id)  # ID for 'O' (outside any entity)\n",
    "label2id[\"O\"] = O_ID\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping\n",
    "\n",
    "print(\"Num BIO classes (incl O):\", len(label2id))\n",
    "pd.Series(label2id).sort_values().head()  # Show a sample of the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdb36e",
   "metadata": {},
   "source": [
    "The code in the previous cell collects all unique entity labels from the dataset's span annotations (such as AGE, EMAIL, TAXNUM) and constructs mappings for BIO tagging. For each entity label, it creates a `B-` (begin) and `I-` (inside) tag, plus a single `O` (outside) tag for non-entity tokens. These mappings (`label2id`, `id2label`) are used to convert entity spans into token-level classification targets for model training. The output displays the discovered labels and the resulting BIO tag-to-ID mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309d516",
   "metadata": {},
   "source": [
    "## 7. Tokenizer & alignment approach\n",
    "\n",
    "- Use `xlm-roberta-base` fast tokenizer with `return_offsets_mapping=True`\n",
    "- For each token span `(s, e)`:\n",
    "  - If (s == e): **special token** â†’ label `-100` (ignored by loss/metrics)\n",
    "  - Else, look at charâ€‘tags in `text[s:e]`:\n",
    "    - Prefer a `B-` if present, else first nonâ€‘`O` tag (an `I-`)\n",
    "  - If none found â†’ `O`\n",
    "- Truncate to `max_length=256` tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d214b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)  # Load fast tokenizer\n",
    "\n",
    "def align_to_bio(example, max_length=256):\n",
    "    \"\"\"Map char-level spans to token-level BIO ids with -100 for specials.\"\"\"\n",
    "    text = example[\"source_text\"]\n",
    "    spans = iter_spans(example)\n",
    "\n",
    "    # Char-level tags initialized to O\n",
    "    char_tags = [\"O\"] * len(text)\n",
    "    for s, e, lab in spans:\n",
    "        if 0 <= s < e <= len(text):\n",
    "            char_tags[s] = f\"B-{lab}\"  # Mark beginning of entity\n",
    "            for i in range(s + 1, e):\n",
    "                char_tags[i] = f\"I-{lab}\"  # Mark inside entity\n",
    "\n",
    "    # Tokenize with offsets\n",
    "    enc = tok(text, return_offsets_mapping=True, truncation=True, max_length=max_length)\n",
    "    labels_tok = []\n",
    "    for (s, e) in enc[\"offset_mapping\"]:\n",
    "        if s == e:\n",
    "            labels_tok.append(-100)  # special tokens -> ignored\n",
    "            continue\n",
    "        window = char_tags[s:e]\n",
    "        if any(t != \"O\" for t in window):\n",
    "            tag = next((t for t in window if t.startswith(\"B-\")),\n",
    "                       next((t for t in window if t != \"O\"), \"O\"))  # Prefer B- tag, else I-\n",
    "        else:\n",
    "            tag = \"O\"\n",
    "        labels_tok.append(label2id.get(tag, O_ID))\n",
    "\n",
    "    # Remove offsets; keep ids, mask, labels\n",
    "    enc.pop(\"offset_mapping\")\n",
    "    enc[\"labels\"] = labels_tok\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c42ab",
   "metadata": {},
   "source": [
    "## 8. Apply alignment to splits\n",
    "\n",
    "We map `align_to_bio` over train/validation and keep only modelâ€‘needed columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88ead30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd9fffe7efa476ca7a34db3919066bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Align validation:   0%|          | 0/82931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 331106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82931\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "cols_keep = [\"input_ids\", \"attention_mask\", \"labels\"]  # Only keep model-required columns\n",
    "ds_tok = DatasetDict({\n",
    "    name: split.map(align_to_bio, remove_columns=split.column_names, desc=f\"Align {name}\")  # Map function to each split\n",
    "               .with_format(\"torch\", columns=cols_keep)  # Set format for PyTorch\n",
    "    for name, split in ds.items()\n",
    "})\n",
    "print(ds_tok)  # Show the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ff5ab",
   "metadata": {},
   "source": [
    "## 9. Save tokenized dataset + label mappings\n",
    "\n",
    "Artifacts:\n",
    "- `data/hf_tokenized/` (HF dataset on disk)\n",
    "- `data/labels.json` (labels, label2id, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838843b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7435a5da0c9e45bc865655b3d165473d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/331106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783cc4a3787f46fa807f1943985d77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/82931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/hf_tokenized and data/labels.json\n"
     ]
    }
   ],
   "source": [
    "# Save label mappings to JSON file\n",
    "(OUT / \"labels.json\").write_text(json.dumps({\n",
    "    \"labels\": labels,\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": id2label\n",
    "}, indent=2))\n",
    "ds_tok.save_to_disk(str(OUT / \"hf_tokenized\"))  # Save tokenized dataset to disk\n",
    "print(\"Saved:\", OUT / \"hf_tokenized\", \"and\", OUT / \"labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9870e",
   "metadata": {},
   "source": [
    "**Versioning for Reproducibility**\n",
    "It's good practice to record the versions of key libraries used for preprocessing and modeling. This ensures that results can be reproduced exactly in the future or by other collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, transformers  # Import libraries to check versions\n",
    "print(f\"datasets: {datasets.__version__}\")  # Print datasets version\n",
    "print(f\"transformers: {transformers.__version__}\")  # Print transformers version\n",
    "print(f\"torch: {torch.__version__}\")  # Print torch version\n",
    "print(f\"pandas: {pd.__version__}\")  # Print pandas version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d42bd9",
   "metadata": {},
   "source": [
    "## 10. Sanity checks\n",
    "\n",
    "- Reload from disk  \n",
    "- Print sizes & a small sample  \n",
    "- Validate label ids / shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9ff7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 331106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82931\n",
      "    })\n",
      "})\n",
      "Example batch shapes: {'input_ids': torch.Size([]), 'attention_mask': torch.Size([]), 'labels': torch.Size([])}\n",
      "Sample record: {'input_ids': tensor([     0,    717,      9,    246,   5303,    100,    201,    927,   8055,\n",
      "         37719,     12,  23356,    678,  23243,     53,   1950,  16762,     99,\n",
      "           209,  22950,     47,  45252,     70, 149849,   4516,  17164,    111,\n",
      "           378,  77804,  24794,    294,  84780,  21130,  61019, 132350,  18044,\n",
      "           454,   2592,    268,      5,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([-100,   40,   40,   40,   40,   40,    4,   24,   24,   24,   40,   40,\n",
      "          40,    8,   28,   14,   34,   40,   17,   37,   40,   40,   40,   40,\n",
      "          40,   40,   40,   40,   40,   40,   40,   40,   40,   40,   40,   40,\n",
      "          40,   40,   40,   40, -100])}\n",
      "Sanity checks passed âœ…\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk  # For loading saved dataset\n",
    "import json  # For reading label mappings\n",
    "try:\n",
    "    reloaded = load_from_disk(str(OUT / \"hf_tokenized\"))  # Reload tokenized dataset from disk\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenized dataset: {e}\")\n",
    "    raise\n",
    "print(reloaded)  # Print dataset info\n",
    "print(\"Example batch shapes:\", {k: v[0].shape if hasattr(v[0], \"shape\") else type(v[0]) for k, v in reloaded[\"train\"][0].items()})  # Show shapes/types\n",
    "print(\"Sample record:\", reloaded[\"train\"][0])  # Show a sample record\n",
    "# Basic assertions\n",
    "assert set([\"input_ids\",\"attention_mask\",\"labels\"]).issubset(reloaded[\"train\"].features), \"Missing features.\"  # Check required features\n",
    "label2id_disk = json.loads((OUT/\"labels.json\").read_text())[\"label2id\"]  # Load label2id from disk\n",
    "assert label2id_disk == label2id, \"label2id mapping mismatch between disk and memory.\"  # Check mapping matches\n",
    "print(\"Sanity checks passed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d26b0",
   "metadata": {},
   "source": [
    "#  \n",
    "Summary: What We Did in This Notebook\n",
    "\n",
    "In this notebook, we prepared a multilingual dataset for training a model to recognize and mask personally identifiable information (PII) in text. Hereâ€™s what we did, step by step:\n",
    "\n",
    "1. **Loaded the Dataset:** We started by loading a large dataset containing sentences in several languages, each with PII labeled.\n",
    "\n",
    "2. **Filtered for Target Languages:** We kept only the samples in English, German, Italian, and French, focusing our work on these four languages.\n",
    "\n",
    "3. **Detected and Normalized Entity Spans:** We identified where PII appears in each sentence and converted the information into a standard format: the start and end positions of each entity, plus its label (like NAME, EMAIL, etc.).\n",
    "\n",
    "4. **Built BIO Tag Mappings:** We created a system to label each word (or token) in a sentence as either the Beginning (B-) or Inside (I-) of an entity, or Outside (O) any entity. This is a common approach for training models to find entities in text.\n",
    "\n",
    "5. **Tokenized and Aligned Labels:** We used a powerful multilingual tokenizer to split sentences into tokens and aligned our BIO labels to these tokens, so the model can learn from them.\n",
    "\n",
    "6. **Saved the Processed Data:** We saved the tokenized dataset and the label mappings to disk, making them ready for model training.\n",
    "\n",
    "7. **Checked Our Work:** We reloaded the saved data, checked its structure, and made sure everything matched up correctly.\n",
    "\n",
    "8. **Recorded Library Versions:** We printed out the versions of the main libraries we used, so anyone can reproduce our results in the future.\n",
    "\n",
    "By following these steps, we turned raw multilingual text data into a format thatâ€™s ready for training a machine learning model to automatically detect and mask PII. This process is essential for building safe, privacy-aware AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pii)",
   "language": "python",
   "name": "pii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
