{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38aa00a",
   "metadata": {},
   "source": [
    "# Converting Multilingual PII Dataset to CoNLL Format\n",
    "\n",
    "This notebook converts the multilingual PII dataset (`ai4privacy/open-pii-masking-500k-ai4privacy`) to CoNLL format, focusing on four languages: English, German, French, and Italian.\n",
    "\n",
    "**Steps:**\n",
    "1. Load and filter the dataset by language\n",
    "2. Convert the filtered data to CoNLL format\n",
    "3. Save and verify the results\n",
    "\n",
    "The CoNLL format will have:\n",
    "- One token per line with its BIO tag\n",
    "- Language information as comments\n",
    "- Blank lines between sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0deb8",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the required libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c3d8cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train', 'validation']\n",
      "\n",
      "Example structure:\n",
      "{'source_text': Value(dtype='string', id=None), 'masked_text': Value(dtype='string', id=None), 'privacy_mask': [{'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'value': Value(dtype='string', id=None), 'label_index': Value(dtype='int64', id=None)}], 'split': Value(dtype='string', id=None), 'uid': Value(dtype='int64', id=None), 'language': Value(dtype='string', id=None), 'region': Value(dtype='string', id=None), 'script': Value(dtype='string', id=None), 'mbert_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'mbert_token_classes': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (only needs to be run once)\n",
    "!pip install -q datasets  # -q flag keeps the output quiet\n",
    "\n",
    "# Import necessary libraries\n",
    "import datasets  # Hugging Face datasets library for loading and processing datasets\n",
    "import os        # For file operations like reading/writing files\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "# This dataset contains text with PII (Personally Identifiable Information) annotations\n",
    "ds = datasets.load_dataset('ai4privacy/open-pii-masking-500k-ai4privacy')\n",
    "\n",
    "# Display basic dataset information\n",
    "print('Available splits:', list(ds.keys()))  # Show train/validation/test splits\n",
    "print('\\nExample structure:')\n",
    "print(ds['train'].features)  # Show what fields are available in each example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9befafe",
   "metadata": {},
   "source": [
    "## 2. Filter Dataset by Language\n",
    "\n",
    "We'll filter the dataset to include only:\n",
    "- English (en)\n",
    "- German (de)\n",
    "- French (fr)\n",
    "- Italian (it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14a96956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train split:\n",
      "  Original: 464,150 examples\n",
      "  Filtered: 331,106 examples (71.3%)\n",
      "\n",
      "validation split:\n",
      "  Original: 116,077 examples\n",
      "  Filtered: 82,931 examples (71.4%)\n",
      "\n",
      "Language distribution in training set:\n",
      "  en: 120,533 examples (36.4%)\n",
      "  fr: 89,670 examples (27.1%)\n",
      "  de: 65,899 examples (19.9%)\n",
      "  it: 55,004 examples (16.6%)\n",
      "\n",
      "Language distribution in training set:\n",
      "  en: 120,533 examples (36.4%)\n",
      "  fr: 89,670 examples (27.1%)\n",
      "  de: 65,899 examples (19.9%)\n",
      "  it: 55,004 examples (16.6%)\n"
     ]
    }
   ],
   "source": [
    "# Define the languages we want to keep in our dataset\n",
    "target_languages = ['en', 'de', 'fr', 'it']  # English, German, French, Italian\n",
    "\n",
    "# Define a function to filter dataset by language\n",
    "def filter_by_languages(dataset, languages):\n",
    "    \"\"\"\n",
    "    Filter a dataset to keep only examples in specified languages.\n",
    "    Args:\n",
    "        dataset: Hugging Face dataset to filter\n",
    "        languages: List of language codes to keep\n",
    "    Returns:\n",
    "        Filtered dataset containing only examples in specified languages\n",
    "    \"\"\"\n",
    "    return dataset.filter(lambda x: x['language'] in languages)\n",
    "\n",
    "# Apply the filtering to all splits (train, validation, etc.)\n",
    "filtered_ds = datasets.DatasetDict({\n",
    "    split: filter_by_languages(ds[split], target_languages)\n",
    "    for split in ds.keys()\n",
    "})\n",
    "\n",
    "# Print statistics to compare original and filtered dataset sizes\n",
    "for split in filtered_ds.keys():\n",
    "    orig_size = len(ds[split])\n",
    "    filt_size = len(filtered_ds[split])\n",
    "    print(f'\\n{split} split:')\n",
    "    print(f'  Original: {orig_size:,} examples')\n",
    "    print(f'  Filtered: {filt_size:,} examples ({filt_size/orig_size*100:.1f}%)')\n",
    "\n",
    "# Show distribution of languages in the filtered training set\n",
    "lang_dist = filtered_ds['train'].to_pandas()['language'].value_counts()\n",
    "print('\\nLanguage distribution in training set:')\n",
    "for lang, count in lang_dist.items():\n",
    "    percentage = count/len(filtered_ds['train'])*100\n",
    "    print(f'  {lang}: {count:,} examples ({percentage:.1f}%)  # {lang}=English/German/French/Italian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901aebe",
   "metadata": {},
   "source": [
    "## 3. Convert to CoNLL Format\n",
    "\n",
    "Now we'll convert the filtered dataset to CoNLL format, where:\n",
    "- Each line contains a token and its BIO tag\n",
    "- Language information is preserved as comments\n",
    "- Sentences are separated by blank lines\n",
    "\n",
    "Example:\n",
    "```\n",
    "# Language: en\n",
    "John B-NAME\n",
    "lives O\n",
    "in O\n",
    "Paris B-LOCATION\n",
    "\n",
    "# Language: de\n",
    "Ich O\n",
    "wohne O\n",
    "in O\n",
    "Berlin B-LOCATION\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0f8999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train.conll with 6,576,194 lines\n",
      "Created validation.conll with 1,644,058 lines\n"
     ]
    }
   ],
   "source": [
    "def text_to_conll(text, spans):\n",
    "    \"\"\"\n",
    "    Convert a text and its PII spans to CoNLL format.\n",
    "    Args:\n",
    "        text: The input text string\n",
    "        spans: List of dictionaries containing PII annotations with 'start', 'end', and 'label' keys\n",
    "    Returns:\n",
    "        List of strings, each string being a CoNLL format line (token BIO-tag)\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the text (simple whitespace tokenization)\n",
    "    tokens = text.split()\n",
    "    # Initialize all tokens with 'O' (Outside) tag\n",
    "    tags = ['O'] * len(tokens)\n",
    "    \n",
    "    def char_to_token_idx(char_pos):\n",
    "        \"\"\"\n",
    "        Convert a character position to a token index.\n",
    "        This is needed because spans have character-level positions,\n",
    "        but we need token-level positions for CoNLL format.\n",
    "        \"\"\"\n",
    "        current_pos = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Find where this token starts in the original text\n",
    "            token_start = text.find(token, current_pos)\n",
    "            token_end = token_start + len(token)\n",
    "            # Check if our target position falls within this token\n",
    "            if token_start <= char_pos < token_end:\n",
    "                return i\n",
    "            current_pos = token_end\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Process each span to assign BIO tags\n",
    "    for span in spans:\n",
    "        # Find which tokens our span starts and ends at\n",
    "        start_idx = char_to_token_idx(span['start'])\n",
    "        end_idx = char_to_token_idx(span['end'])\n",
    "        \n",
    "        if start_idx is not None:\n",
    "            # B- prefix for the first token of the entity\n",
    "            tags[start_idx] = f'B-{span[\"label\"].upper()}'\n",
    "            \n",
    "            # I- prefix for any subsequent tokens of the same entity\n",
    "            if end_idx and end_idx > start_idx:\n",
    "                for i in range(start_idx + 1, end_idx + 1):\n",
    "                    if i < len(tags):  # Safety check\n",
    "                        tags[i] = f'I-{span[\"label\"].upper()}'\n",
    "\n",
    "    # Step 3: Create the CoNLL format lines\n",
    "    return [f'{token} {tag}' for token, tag in zip(tokens, tags)]\n",
    "\n",
    "def create_conll_file(dataset, split, output_path):\n",
    "    \"\"\"\n",
    "    Create a CoNLL format file from a dataset split.\n",
    "    Args:\n",
    "        dataset: Hugging Face dataset\n",
    "        split: Which split to process ('train', 'validation', etc.)\n",
    "        output_path: Where to save the CoNLL file\n",
    "    Returns:\n",
    "        Number of lines written to the file\n",
    "    \"\"\"\n",
    "    all_lines = []\n",
    "    \n",
    "    # Process each example in the dataset\n",
    "    for example in dataset[split]:\n",
    "        # Add a comment line with language information\n",
    "        all_lines.append(f'# Language: {example[\"language\"]}')\n",
    "        \n",
    "        # Convert this example to CoNLL format\n",
    "        conll_lines = text_to_conll(example['source_text'], example['spans'])\n",
    "        all_lines.extend(conll_lines)\n",
    "        \n",
    "        # Add blank line to separate sentences (CoNLL format requirement)\n",
    "        all_lines.append('')\n",
    "    \n",
    "    # Write all lines to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_lines))\n",
    "    \n",
    "    return len(all_lines)  # Return number of lines for statistics\n",
    "\n",
    "# Define output file paths\n",
    "train_path = 'train.conll'\n",
    "val_path = 'validation.conll'\n",
    "\n",
    "# Create CoNLL files for both splits\n",
    "train_lines = create_conll_file(filtered_ds, 'train', train_path)\n",
    "val_lines = create_conll_file(filtered_ds, 'validation', val_path)\n",
    "\n",
    "# Print statistics about the created files\n",
    "print(f'Created {train_path} with {train_lines:,} lines')\n",
    "print(f'Created {val_path} with {val_lines:,} lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9f42b",
   "metadata": {},
   "source": [
    "## 4. Verify the Output\n",
    "\n",
    "Let's check the generated CoNLL files to ensure they're formatted correctly and contain the expected information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3115edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from train.conll:\n",
      "First 20 lines of train.conll:\n",
      "--------------------------------------------------\n",
      "# Language: en\n",
      "To-do O\n",
      "list O\n",
      "for O\n",
      "4th B-DATE\n",
      "August I-DATE\n",
      "1942: I-DATE\n",
      "meet O\n",
      "with O\n",
      "Brandy B-GIVENNAME\n",
      "Haroon B-SURNAME\n",
      "at O\n",
      "10:17 B-TIME\n",
      "to O\n",
      "discuss O\n",
      "the O\n",
      "volunteer O\n",
      "service O\n",
      "record O\n",
      "of O\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample from validation.conll:\n",
      "First 20 lines of validation.conll:\n",
      "--------------------------------------------------\n",
      "# Language: fr\n",
      "Ma O\n",
      "mère O\n",
      "Astrit B-GIVENNAME\n",
      "Nani O\n",
      "Kofi B-SURNAME\n",
      "est O\n",
      "née O\n",
      "à O\n",
      "Ruswil B-CITY\n",
      "en O\n",
      "février/75 B-DATE\n",
      "\n",
      "# Language: de\n",
      "15. B-DATE\n",
      "November I-DATE\n",
      "1942: I-DATE\n",
      "Datum O\n",
      "der O\n",
      "Einreichung O\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def peek_file(filepath, num_lines=20):\n",
    "    \"\"\"\n",
    "    Display the first few lines of a file with nice formatting.\n",
    "    Args:\n",
    "        filepath: Path to the file to read\n",
    "        num_lines: Number of lines to display (default: 20)\n",
    "    \"\"\"\n",
    "    print(f'First {num_lines} lines of {filepath}:')\n",
    "    print('-' * 50)  # Separator line for readability\n",
    "    \n",
    "    # Read and display the lines\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            # Strip trailing whitespace for clean display\n",
    "            print(line.rstrip())\n",
    "    \n",
    "    print('-' * 50)  # Closing separator line\n",
    "\n",
    "# Check samples from both training and validation files\n",
    "for filepath in [train_path, val_path]:\n",
    "    print(f'\\nSample from {filepath}:')\n",
    "    peek_file(filepath)  # Show first 20 lines by default\n",
    "\n",
    "# The output should show:\n",
    "# 1. Language comments (# Language: xx)\n",
    "# 2. Tokens with their BIO tags\n",
    "# 3. Blank lines between sentences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
