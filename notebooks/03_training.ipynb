{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b44d4",
   "metadata": {},
   "source": [
    "# 03 - Model Training\n",
    "\n",
    "In this notebook, we fine-tune the multilingual `xlm-roberta-base` model for token-level classification (NER-like task) on our preprocessed PII dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae98e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (0.34.4)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.7)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp->datasets>=2.0.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp->datasets>=2.0.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorForTokenClassification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a95fc2",
   "metadata": {},
   "source": [
    "## 1. Load tokenized datasets and labels\n",
    "\n",
    " In this step, we load the dataset that was tokenized and aligned in Notebook 2 - `02_preprocessing.ipynb`.\n",
    " We also reload the label mappings (`label2id`, `id2label`) that we saved as JSON.\n",
    " This ensures our model knows how to map between numeric IDs and string labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae74f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 331106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82931\n",
      "    })\n",
      "})\n",
      "Number of labels: 41\n",
      "Sample label mapping: [('B-AGE', 0), ('B-BUILDINGNUM', 1), ('B-CITY', 2), ('B-CREDITCARDNUMBER', 3), ('B-DATE', 4), ('B-DRIVERLICENSENUM', 5), ('B-EMAIL', 6), ('B-GENDER', 7), ('B-GIVENNAME', 8), ('B-IDCARDNUM', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenized dataset from disk\n",
    "ds = load_from_disk(\"data/hf_tokenized\")\n",
    "\n",
    "# Load label metadata\n",
    "meta = json.loads(Path(\"data/labels.json\").read_text())\n",
    "label2id = meta[\"label2id\"]\n",
    "id2label = {int(v): k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Dataset splits:\", ds)\n",
    "print(\"Number of labels:\", len(label2id))\n",
    "print(\"Sample label mapping:\", list(label2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013db24",
   "metadata": {},
   "source": [
    "# 2. Load tokenizer and initialize model\n",
    "\n",
    "We load the same tokenizer (`xlm-roberta-base`) that we used during preprocessing. Then we initialize a fresh token classification model, specifying the number of labels and the label ↔ id mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2d151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c5615c1d745dc83b642cd4124a4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "MODEL = \"xlm-roberta-base\"\n",
    "\n",
    "# Reload tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "# Load metadata (labels and mappings)\n",
    "with open(\"data/labels.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "label2id = meta[\"label2id\"]\n",
    "id2label = {int(v): k for k, v in label2id.items()}\n",
    "\n",
    "# Initialize model for token classification\n",
    "print(\"Initializing model...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a75beb",
   "metadata": {},
   "source": [
    "## 3. Load preprocessed dataset and prepare data collator\n",
    "\n",
    "Load dataset that we saved in preprocessing step (tokenized + aligned labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbc0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 331106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82931\n",
      "    })\n",
      "})\n",
      "Number of labels: 41\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "ds = load_from_disk(\"data/hf_tokenized\")\n",
    "\n",
    "# Load meta information about labels (id2label and label2id)\n",
    "meta = json.loads(Path(\"data/labels.json\").read_text())\n",
    "label2id = meta[\"label2id\"]\n",
    "id2label = {int(v): k for k, v in label2id.items()}\n",
    "\n",
    "# Load tokenizer again (same as model)\n",
    "tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", use_fast=True)\n",
    "\n",
    "# Data collator handles dynamic padding and batches\n",
    "collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "print(\"Dataset:\", ds)\n",
    "print(\"Number of labels:\", len(label2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9319a",
   "metadata": {},
   "source": [
    "This dataset contains two splits: `train` (331,106 samples) and `validation` (82,931 samples). Each sample includes `input_ids`, `attention_mask`, and `labels`. There are 41 unique label classes for token classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7394b4",
   "metadata": {},
   "source": [
    "# 4. Load model for token classification\n",
    "\n",
    "We load the XLM-R model with a classification head for token-level tasks. The number of labels is determined from the label mapping we loaded in Cell 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5636f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 41 labels.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"Model loaded with\", len(label2id), \"labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d6066",
   "metadata": {},
   "source": [
    "## 5. Setup data collator and evaluation metric\n",
    "\n",
    "The data collator takes care of batching inputs together and properly padding them. For token classification, Hugging Face provides a dedicated collator.\n",
    "\n",
    "We also load the `seqeval` metric, which is a standard for NER-like tasks.\n",
    "\n",
    "It computes precision, recall, and F1 based on entity spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda389ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l  Preparing metadata (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from seqeval) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from seqeval) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/envs/pii/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "\u001b[33m  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for seqeval (setup.py) ... \u001b[?25lBuilding wheels for collected packages: seqeval\n",
      "\u001b[33m  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for seqeval (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16249 sha256=a1fe5828c4268480ca4ce37bd5720868b60f43119c899a3c02b249463ea94491\n",
      "  Stored in directory: /Users/arifhizlan/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16249 sha256=a1fe5828c4268480ca4ce37bd5720868b60f43119c899a3c02b249463ea94491\n",
      "  Stored in directory: /Users/arifhizlan/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "%pip install seqeval\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db83baf",
   "metadata": {},
   "source": [
    "## 6. Define `compute_metrics` function\n",
    "\n",
    "Define compute_metrics function to evaluate model performance using seqeval (standard for sequence labeling / NER tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a66d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=-1)  # take highest scoring label per token\n",
    "    labels = p.label_ids\n",
    "\n",
    "    true_predictions, true_labels = [], []\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        pred_labels, gold_labels = [], []\n",
    "        for p_i, l_i in zip(pred_seq, label_seq):\n",
    "            if l_i == -100:  # ignore padding tokens\n",
    "                continue\n",
    "            pred_labels.append(id2label[p_i])\n",
    "            gold_labels.append(id2label[l_i])\n",
    "        true_predictions.append(pred_labels)\n",
    "        true_labels.append(gold_labels)\n",
    "\n",
    "    # Compute precision, recall, F1\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436f2fc",
   "metadata": {},
   "source": [
    "## 7. Training arguments\n",
    "Here we define hyperparameters and training setup for fine-tuning XLM-R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5bf9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"runs/xlmr-baseline\",           # where checkpoints/logs are saved\n",
    "    learning_rate=3e-5,                        # optimizer learning rate\n",
    "    per_device_train_batch_size=16,            # batch size for training\n",
    "    per_device_eval_batch_size=16,             # batch size for evaluation\n",
    "    num_train_epochs=3,                        # number of epochs to train\n",
    "    evaluation_strategy=\"epoch\",               # when to run eval\n",
    "    save_strategy=\"epoch\",                     # when to save checkpoints\n",
    "    logging_steps=50,                          # how often to log\n",
    "    seed=SEED,                                 # random seed for reproducibility\n",
    "    report_to=\"none\"                           # disables reporting to external services (e.g., wandb)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952487c",
   "metadata": {},
   "source": [
    "## 8. Define Trainer\n",
    "\n",
    "The Hugging Face `Trainer` class brings everything together:\n",
    "- The **model** (`xlm-roberta-base` fine-tuning head for token classification).\n",
    "- The **training arguments** (batch size, learning rate, logging).\n",
    "- The **datasets** (`train` and `validation`).\n",
    "- The **data collator** (ensures batches are padded correctly).\n",
    "- The **metrics function** (computes precision, recall, F1 using seqeval).\n",
    "\n",
    "This allows us to start training with a single `.train()` call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Trainer\n",
    "# Build Hugging Face Trainer for token classification\n",
    "trainer = Trainer(\n",
    "    model=model,                      # The XLM-R model with token classification head\n",
    "    args=args,                        # Training arguments (batch size, epochs, etc.)\n",
    "    train_dataset=ds[\"train\"],        # Training split of the dataset\n",
    "    eval_dataset=ds[\"validation\"],    # Validation split for evaluation\n",
    "    tokenizer=tok,                    # Tokenizer for preprocessing\n",
    "    data_collator=collator,           # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics (precision, recall, F1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12860df5",
   "metadata": {},
   "source": [
    "## 9. Inspect and Run Training\n",
    "\n",
    "We first initialize the `Trainer` object.  \n",
    "It encapsulates our entire training pipeline.  \n",
    "Before launching training, we can print or inspect the object to confirm everything looks correct.  \n",
    "Then, calling `trainer.train()` will start the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9e5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer.Trainer object at 0x1524b9c90>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2172756f68491284fb8b25437f8c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pii/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1575: UserWarning: cumsum_out_mps supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/UnaryOps.mm:425.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 4.41 GB, other allocations: 2.18 GB, max allowed: 6.80 GB). Tried to allocate 732.43 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(trainer)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/transformers/trainer.py:2341\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2339\u001b[39m         grad_norm = _grad_norm\n\u001b[32m-> \u001b[39m\u001b[32m2341\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2345\u001b[39m optimizer_was_run = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.optimizer_step_was_skipped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/accelerate/optimizer.py:170\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m instance._step_count += \u001b[32m1\u001b[39m\n\u001b[32m     74\u001b[39m wrapped = func.\u001b[34m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/adamw.py:187\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    174\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    176\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    177\u001b[39m         group,\n\u001b[32m    178\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m         state_steps,\n\u001b[32m    185\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/adamw.py:339\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    337\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/pii/lib/python3.11/site-packages/torch/optim/adamw.py:470\u001b[39m, in \u001b[36m_single_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    468\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m         denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / bias_correction2_sqrt).add_(eps)\n\u001b[32m    472\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 4.41 GB, other allocations: 2.18 GB, max allowed: 6.80 GB). Tried to allocate 732.43 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Inspect Trainer setup\n",
    "print(trainer)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c42b97",
   "metadata": {},
   "source": [
    "## 10. Evaluate and Save the Model\n",
    "\n",
    "After training finishes, we:\n",
    "1. Run evaluation on the validation set to get metrics (precision, recall, F1).\n",
    "2. Save the fine-tuned model for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Save fine-tuned model and tokenizer\n",
    "trainer.save_model(\"model_xlmr_openpii\")\n",
    "tok.save_pretrained(\"model_xlmr_openpii\")\n",
    "\n",
    "print(\"Model and tokenizer saved to: model_xlmr_openpii\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pii)",
   "language": "python",
   "name": "pii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
