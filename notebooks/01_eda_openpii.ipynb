{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4d2b00",
   "metadata": {},
   "source": [
    "## EDA Workflow Overview\n",
    "This notebook will guide you through:\n",
    "- Loading and inspecting the dataset structure.\n",
    "- Exploring label distributions and entity types.\n",
    "- Visualizing key statistics (e.g., label frequencies, text lengths).\n",
    "- Highlighting any data quality or preprocessing considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print package versions for reproducibility\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2c368",
   "metadata": {},
   "source": [
    "# Multilingual PII NER: Exploratory Data Analysis (EDA)\n",
    "This notebook explores the dataset used for multilingual Personally Identifiable Information (PII) Named Entity Recognition (NER).\n",
    " \n",
    "**Project Goal:** Build and evaluate models for detecting and masking PII entities in multilingual text.\n",
    " \n",
    "**Dataset:**\n",
    "- Contains annotated text samples with PII spans and labels.\n",
    "- Multilingual, with various entity types (e.g., NAME, LOCATION, EMAIL, etc.).\n",
    " \n",
    "**Notebook Objectives:**\n",
    "1. Understand the structure and distribution of the data.\n",
    "2. Visualize label frequencies and data characteristics.\n",
    "3. Identify potential issues or preprocessing needs before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60533eac",
   "metadata": {},
   "source": [
    "# 01 — Exploratory Data Analysis (EDA) of OpenPII Dataset\n",
    "\n",
    "This notebook provides an initial exploration of the ai4privacy/open-pii-masking-500k-ai4privacy dataset. We will examine the structure, sample records, label distribution, and language coverage to better understand the data before preprocessing and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771bb5d",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data\n",
    "\n",
    "We will install required packages and load the OpenPII dataset from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2b6ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dce7f664fdd4d26a98b8358907a465c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d84ba00bcfb4a2eae864eac5a9675d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6717518605394317b6e4f910f9849cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/464150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019f4c62ceed49ca9a230f586c872f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/116077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
       "        num_rows: 464150\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes'],\n",
       "        num_rows: 116077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages if running in a fresh environment\n",
    "%pip install datasets pandas matplotlib seaborn --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "ds = load_dataset('ai4privacy/open-pii-masking-500k-ai4privacy')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa53e2",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The `DatasetDict` object contains two splits: `train` and `validation`. Each split is a `Dataset` with 10 features (columns), such as `source_text`, `masked_text`, `privacy_mask`, and metadata like `language` and `region`. The `train` split has 464,150 examples, while the `validation` split has 116,077 examples. This structure allows for easy access and manipulation of the data for training and evaluating models.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f069eb",
   "metadata": {},
   "source": [
    "## 2. Sample Records\n",
    "\n",
    "Let's look at a few samples from the training set to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720ae3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>privacy_mask</th>\n",
       "      <th>split</th>\n",
       "      <th>uid</th>\n",
       "      <th>language</th>\n",
       "      <th>region</th>\n",
       "      <th>script</th>\n",
       "      <th>mbert_tokens</th>\n",
       "      <th>mbert_token_classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20:10:26 Venanzius Höttermann Revés యొక్క వివా...</td>\n",
       "      <td>[TIME_1] [GIVENNAME_1] [SURNAME_1] యొక్క వివాహ...</td>\n",
       "      <td>[{'label': 'TIME', 'start': 0, 'end': 8, 'valu...</td>\n",
       "      <td>train</td>\n",
       "      <td>5387382</td>\n",
       "      <td>te</td>\n",
       "      <td>IN</td>\n",
       "      <td>Telu</td>\n",
       "      <td>[20, :, 10, :, 26, Ve, ##nan, ##ziu, ##s, H, #...</td>\n",
       "      <td>[B-TIME, I-TIME, I-TIME, I-TIME, I-TIME, B-GIV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Branislavka: 'Sí, por favor. ¿Cuánta cera de s...</td>\n",
       "      <td>[GIVENNAME_1]: 'Sí, por favor. ¿Cuánta cera de...</td>\n",
       "      <td>[{'label': 'GIVENNAME', 'start': 0, 'end': 11,...</td>\n",
       "      <td>train</td>\n",
       "      <td>5401531</td>\n",
       "      <td>es</td>\n",
       "      <td>MX</td>\n",
       "      <td>Latn</td>\n",
       "      <td>[Br, ##ani, ##slav, ##ka, :, ', S, ##í, ,, por...</td>\n",
       "      <td>[B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To-do list for 4th August 1942: meet with Bran...</td>\n",
       "      <td>To-do list for [DATE_1]: meet with [GIVENNAME_...</td>\n",
       "      <td>[{'label': 'DATE', 'start': 15, 'end': 30, 'va...</td>\n",
       "      <td>train</td>\n",
       "      <td>5387389</td>\n",
       "      <td>en</td>\n",
       "      <td>CA</td>\n",
       "      <td>Latn</td>\n",
       "      <td>[To, -, do, list, for, 4th, August, 1942, :, m...</td>\n",
       "      <td>[O, O, O, O, O, B-DATE, I-DATE, I-DATE, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Igorche Ramtin Eshekary will need to bring the...</td>\n",
       "      <td>[GIVENNAME_1] [SURNAME_1] will need to bring t...</td>\n",
       "      <td>[{'label': 'GIVENNAME', 'start': 0, 'end': 14,...</td>\n",
       "      <td>train</td>\n",
       "      <td>5406386</td>\n",
       "      <td>en</td>\n",
       "      <td>GB</td>\n",
       "      <td>Latn</td>\n",
       "      <td>[Igor, ##che, Ram, ##tin, Es, ##he, ##kar, ##y...</td>\n",
       "      <td>[B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shola Kenzi Zimeri used 0111-284596398 to sche...</td>\n",
       "      <td>[GIVENNAME_1] [SURNAME_1] used [TELEPHONENUM_1...</td>\n",
       "      <td>[{'label': 'GIVENNAME', 'start': 0, 'end': 11,...</td>\n",
       "      <td>train</td>\n",
       "      <td>5402211</td>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>Latn</td>\n",
       "      <td>[S, ##hola, Ken, ##zi, Zi, ##meri, used, 011, ...</td>\n",
       "      <td>[B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         source_text  \\\n",
       "0  20:10:26 Venanzius Höttermann Revés యొక్క వివా...   \n",
       "1  Branislavka: 'Sí, por favor. ¿Cuánta cera de s...   \n",
       "2  To-do list for 4th August 1942: meet with Bran...   \n",
       "3  Igorche Ramtin Eshekary will need to bring the...   \n",
       "4  Shola Kenzi Zimeri used 0111-284596398 to sche...   \n",
       "\n",
       "                                         masked_text  \\\n",
       "0  [TIME_1] [GIVENNAME_1] [SURNAME_1] యొక్క వివాహ...   \n",
       "1  [GIVENNAME_1]: 'Sí, por favor. ¿Cuánta cera de...   \n",
       "2  To-do list for [DATE_1]: meet with [GIVENNAME_...   \n",
       "3  [GIVENNAME_1] [SURNAME_1] will need to bring t...   \n",
       "4  [GIVENNAME_1] [SURNAME_1] used [TELEPHONENUM_1...   \n",
       "\n",
       "                                        privacy_mask  split      uid language  \\\n",
       "0  [{'label': 'TIME', 'start': 0, 'end': 8, 'valu...  train  5387382       te   \n",
       "1  [{'label': 'GIVENNAME', 'start': 0, 'end': 11,...  train  5401531       es   \n",
       "2  [{'label': 'DATE', 'start': 15, 'end': 30, 'va...  train  5387389       en   \n",
       "3  [{'label': 'GIVENNAME', 'start': 0, 'end': 14,...  train  5406386       en   \n",
       "4  [{'label': 'GIVENNAME', 'start': 0, 'end': 11,...  train  5402211       en   \n",
       "\n",
       "  region script                                       mbert_tokens  \\\n",
       "0     IN   Telu  [20, :, 10, :, 26, Ve, ##nan, ##ziu, ##s, H, #...   \n",
       "1     MX   Latn  [Br, ##ani, ##slav, ##ka, :, ', S, ##í, ,, por...   \n",
       "2     CA   Latn  [To, -, do, list, for, 4th, August, 1942, :, m...   \n",
       "3     GB   Latn  [Igor, ##che, Ram, ##tin, Es, ##he, ##kar, ##y...   \n",
       "4     US   Latn  [S, ##hola, Ken, ##zi, Zi, ##meri, used, 011, ...   \n",
       "\n",
       "                                 mbert_token_classes  \n",
       "0  [B-TIME, I-TIME, I-TIME, I-TIME, I-TIME, B-GIV...  \n",
       "1  [B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...  \n",
       "2  [O, O, O, O, O, B-DATE, I-DATE, I-DATE, O, O, ...  \n",
       "3  [B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...  \n",
       "4  [B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, I-GIVE...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ds['train'][:10])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb01718",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Language Counts (Train Split)\n",
    "\n",
    "We count the number of examples per language (within our filtered training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    120533\n",
       "fr     89670\n",
       "de     65899\n",
       "es     62586\n",
       "it     55004\n",
       "hi     27025\n",
       "te     22152\n",
       "nl     21281\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# language counts\n",
    "train = ds['train']\n",
    "pd.Series([ex[\"language\"] for ex in train]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631285af",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Label Frequency (Train Split)\n",
    "\n",
    "Each example contains span annotations in `span_labels`, where each item is\n",
    "`[start_char, end_char, label_name]`.\n",
    "\n",
    "Here we flatten all labels in the training split and count their frequency to\n",
    "get a sense of which PII entity types are most/least common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc60fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIVENNAME           347442\n",
       "SURNAME             134026\n",
       "CITY                 76605\n",
       "TELEPHONENUM         73662\n",
       "TIME                 64456\n",
       "DATE                 54438\n",
       "EMAIL                53994\n",
       "STREET               49919\n",
       "BUILDINGNUM          43703\n",
       "IDCARDNUM            39126\n",
       "TITLE                37690\n",
       "AGE                  25914\n",
       "ZIPCODE              18597\n",
       "PASSPORTNUM          17699\n",
       "TAXNUM               12402\n",
       "SEX                  11404\n",
       "CREDITCARDNUMBER     10317\n",
       "SOCIALNUM            10020\n",
       "GENDER                9609\n",
       "DRIVERLICENSENUM      9533\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# label freq (flatten span_labels)\n",
    "def label_counts(split):\n",
    "    c = Counter()\n",
    "    for ex in split:\n",
    "        for s in ex[\"privacy_mask\"] or []:\n",
    "            c[s['label']] += 1\n",
    "    # Convert Counter to pandas Series, sort by frequency (descending), and return\n",
    "    return pd.Series(c).sort_values(ascending=False)\n",
    "\n",
    "# Show label counts for the train split\n",
    "label_counts(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b87b04",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Span Length Distribution (Characters)\n",
    "\n",
    "We look at **character-length** of each annotated span (i.e., `end - start`)\n",
    "over a small subset (first 5,000 examples for speed). The summary statistics\n",
    "(`describe()`) give us min/median/mean/max. This is useful to:\n",
    "- anticipate typical token lengths after tokenization,\n",
    "- consider window sizes and truncation for model training,\n",
    "- inform decisions about augmentation or heuristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4231fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12576.000000\n",
       "mean         9.928833\n",
       "std          5.546272\n",
       "min          1.000000\n",
       "25%          6.000000\n",
       "50%          9.000000\n",
       "75%         14.000000\n",
       "max         56.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cell 5: span length distribution (chars)\n",
    "lens = []\n",
    "for ex in train.select(range(5000)):\n",
    "    for s in ex[\"span_labels\"] or []:\n",
    "        lens.append(s[1]-s[0])\n",
    "pd.Series(lens).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b14340",
   "metadata": {},
   "source": [
    "The span length statistics show that most annotated spans are relatively short, with a median of 9 and a mean of about 10 characters. The majority (75%) are 14 characters or fewer, but some reach up to 56 characters. This suggests that most PII entities are concise, but models should be prepared to handle longer spans. These insights help inform tokenization strategies, model input window sizes, and potential data augmentation approaches to ensure robust handling of both short and long spans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f4d8e",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "- The dataset contains multilingual text annotated for various PII entity types.\n",
    "- Label frequencies and data characteristics have been visualized and explored.\n",
    "- No major data quality issues were found, needed further preprocessing will be done in `02_preprocessing.ipynb` for model training.\n",
    " \n",
    "**Next Steps:**\n",
    "- Preprocess and tokenize the data for model input.\n",
    "- Train and evaluate NER models.\n",
    "- Analyze model performance and iterate as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
