{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60533eac",
   "metadata": {},
   "source": [
    "\n",
    "# EDA: Multilingual PII Dataset (ai4privacy/open-pii-masking-500k-ai4privacy)\n",
    "\n",
    "This notebook performs a **quick exploratory data analysis (EDA)** on the\n",
    "Hugging Face dataset **`ai4privacy/open-pii-masking-500k-ai4privacy`**,\n",
    "focusing on the languages **English (en)**, **German (de)**, **Italian (it)**,\n",
    "and **French (fr)**.\n",
    "\n",
    "**Goals**\n",
    "- Verify we can load the dataset successfully.\n",
    "- Inspect the size of the splits for the selected languages.\n",
    "- Check label distribution (which PII types are most common?).\n",
    "- Look at span-length statistics to understand typical entity sizes.\n",
    "\n",
    "> Tip: Run cells top-to-bottom. If this is your first time using the dataset,\n",
    "> the first cell that loads it will download and cache it (this can take a bit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771bb5d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Settings\n",
    "\n",
    "We import the core libraries for data handling and the Hugging Face `datasets`\n",
    "loader. We also fix a random seed for reproducibility of any sampling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: imports & settings\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, json, random\n",
    "from datasets import load_dataset\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f069eb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load Dataset & Filter Languages\n",
    "\n",
    "- We load the dataset with `load_dataset`.\n",
    "- We filter to our target languages: **en/de/it/fr** using the dataset's\n",
    "  `language` field.\n",
    "- The dataset exposes splits like `train` and `validate` (naming may vary by\n",
    "  dataset), so we use those and apply the language filter.\n",
    "- The last line returns the sizes of the filtered splits for a quick sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ae3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: load subset (en/de/it/fr)\n",
    "LANGS = {\"en\",\"de\",\"it\",\"fr\"}\n",
    "ds = load_dataset(\"ai4privacy/open-pii-masking-500k-ai4privacy\")\n",
    "train = ds[\"train\"].filter(lambda x: x[\"language\"] in LANGS)\n",
    "valid = ds[\"validate\"].filter(lambda x: x[\"language\"] in LANGS)\n",
    "len(train), len(valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb01718",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Language Counts (Train Split)\n",
    "\n",
    "We count the number of examples per language (within our filtered training data).\n",
    "This helps us see whether the dataset is balanced across **en/de/it/fr** or\n",
    "dominated by a single language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: language counts\n",
    "pd.Series([ex[\"language\"] for ex in train]).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631285af",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Label Frequency (Train Split)\n",
    "\n",
    "Each example contains span annotations in `span_labels`, where each item is\n",
    "`[start_char, end_char, label_name]`.\n",
    "\n",
    "Here we flatten all labels in the training split and count their frequency to\n",
    "get a sense of which PII entity types are most/least common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: label freq (flatten span_labels)\n",
    "from collections import Counter\n",
    "def label_counts(split):\n",
    "    c = Counter()\n",
    "    for ex in split:\n",
    "        for s in ex[\"span_labels\"] or []:\n",
    "            c[s[2]] += 1\n",
    "    return pd.Series(c).sort_values(ascending=False)\n",
    "\n",
    "label_counts(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b87b04",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Span Length Distribution (Characters)\n",
    "\n",
    "We look at **character-length** of each annotated span (i.e., `end - start`)\n",
    "over a small subset (first 5,000 examples for speed). The summary statistics\n",
    "(`describe()`) give us min/median/mean/max. This is useful to:\n",
    "- anticipate typical token lengths after tokenization,\n",
    "- consider window sizes and truncation for model training,\n",
    "- inform decisions about augmentation or heuristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4231fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 5: span length distribution (chars)\n",
    "lens = []\n",
    "for ex in train.select(range(5000)):\n",
    "    for s in ex[\"span_labels\"] or []:\n",
    "        lens.append(s[1]-s[0])\n",
    "pd.Series(lens).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508aba58",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "\n",
    "- Plot distributions (e.g., histograms of span lengths) if needed.\n",
    "- Inspect examples by label for quality checks.\n",
    "- Proceed to **token-level label alignment** (BIO/BILOU) and baseline fine-tuning\n",
    "  with a multilingual model (e.g., `xlm-roberta-base`).\n",
    "\n",
    "When you're ready, we can add a **preprocessing script** and **training script**\n",
    "to this repo.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
